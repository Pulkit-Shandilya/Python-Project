{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f3a6e9",
   "metadata": {},
   "source": [
    "# Scikit-Learn Practice Questions\n",
    "\n",
    "This notebook covers essential scikit-learn operations including train-test split, model training, evaluation, and various machine learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6cdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.datasets import make_classification, make_regression, load_iris, load_boston\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Scikit-learn version: {__import__('sklearn').__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf40df",
   "metadata": {},
   "source": [
    "## 1. Train-Test Split Demonstration\n",
    "\n",
    "Demonstrate various train-test split strategies and their importance in machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c64eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset for classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
    "                          n_redundant=5, n_classes=3, random_state=42)\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print()\n",
    "\n",
    "# Basic train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Basic Train-Test Split (80-20):\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"Training set class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test set class distribution: {np.bincount(y_test)}\")\n",
    "print()\n",
    "\n",
    "# Stratified split to maintain class proportions\n",
    "X_train_strat, X_test_strat, y_train_strat, y_test_strat = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "print(\"Stratified Train-Test Split (80-20):\")\n",
    "print(f\"Training set class distribution: {np.bincount(y_train_strat)}\")\n",
    "print(f\"Test set class distribution: {np.bincount(y_test_strat)}\")\n",
    "print()\n",
    "\n",
    "# Train-Validation-Test split (60-20-20)\n",
    "X_temp, X_test_final, y_temp, y_test_final = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)  # 0.25 of 0.8 = 0.2\n",
    "\n",
    "print(\"Train-Validation-Test Split (60-20-20):\")\n",
    "print(f\"Training set: {X_train_final.shape[0]} samples ({X_train_final.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test_final.shape[0]} samples ({X_test_final.shape[0]/len(X)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Visualize the splits\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original distribution\n",
    "axes[0].bar(range(len(np.bincount(y))), np.bincount(y))\n",
    "axes[0].set_title('Original Class Distribution')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Basic split\n",
    "width = 0.35\n",
    "x = np.arange(len(np.unique(y)))\n",
    "axes[1].bar(x - width/2, np.bincount(y_train), width, label='Train', alpha=0.7)\n",
    "axes[1].bar(x + width/2, np.bincount(y_test), width, label='Test', alpha=0.7)\n",
    "axes[1].set_title('Basic Split Distribution')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend()\n",
    "\n",
    "# Stratified split\n",
    "axes[2].bar(x - width/2, np.bincount(y_train_strat), width, label='Train', alpha=0.7)\n",
    "axes[2].bar(x + width/2, np.bincount(y_test_strat), width, label='Test', alpha=0.7)\n",
    "axes[2].set_title('Stratified Split Distribution')\n",
    "axes[2].set_xlabel('Class')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Demonstrate different random states\n",
    "print(\"Effect of different random states:\")\n",
    "for i, random_state in enumerate([42, 100, 999]):\n",
    "    X_train_rs, X_test_rs, y_train_rs, y_test_rs = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state)\n",
    "    print(f\"Random state {random_state}: Train class dist = {np.bincount(y_train_rs)}, \"\n",
    "          f\"Test class dist = {np.bincount(y_test_rs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62345a",
   "metadata": {},
   "source": [
    "## 2. Classification Example with Model Training\n",
    "\n",
    "Build and evaluate classification models using the train-test split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the iris dataset for a more realistic example\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "\n",
    "print(\"Iris Dataset Information:\")\n",
    "print(f\"Features: {iris.feature_names}\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "print(f\"Dataset shape: {X_iris.shape}\")\n",
    "print()\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, stratify=y_iris, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print()\n",
    "\n",
    "# Train multiple classifiers\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    # Train the model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': clf,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Detailed evaluation for the best model\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
    "best_model = results[best_model_name]['model']\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, best_predictions, target_names=iris.target_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Cross-validation scores\n",
    "print(\"\\nCross-Validation Scores (5-fold):\")\n",
    "for name, clf in classifiers.items():\n",
    "    cv_scores = cross_val_score(clf, X_iris, y_iris, cv=5)\n",
    "    print(f\"{name}: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abc8a99",
   "metadata": {},
   "source": [
    "## 3. Regression Example with Model Training\n",
    "\n",
    "Build and evaluate regression models using train-test split methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regression dataset\n",
    "X_reg, y_reg = make_regression(n_samples=500, n_features=10, n_informative=8, \n",
    "                              noise=0.1, random_state=42)\n",
    "\n",
    "print(\"Regression Dataset Information:\")\n",
    "print(f\"Features shape: {X_reg.shape}\")\n",
    "print(f\"Target shape: {y_reg.shape}\")\n",
    "print(f\"Target mean: {y_reg.mean():.2f}\")\n",
    "print(f\"Target std: {y_reg.std():.2f}\")\n",
    "print()\n",
    "\n",
    "# Split the data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train_reg.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_reg.shape[0]} samples\")\n",
    "print()\n",
    "\n",
    "# Train regression models\n",
    "regressors = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "reg_results = {}\n",
    "\n",
    "for name, reg in regressors.items():\n",
    "    # Train the model\n",
    "    reg.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_reg = reg.predict(X_test_reg)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "    \n",
    "    # Store results\n",
    "    reg_results[name] = {\n",
    "        'model': reg,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'predictions': y_pred_reg\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  R² Score: {r2:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize predictions vs actual values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "for i, (name, results) in enumerate(reg_results.items()):\n",
    "    y_pred = results['predictions']\n",
    "    r2 = results['r2']\n",
    "    \n",
    "    axes[i].scatter(y_test_reg, y_pred, alpha=0.6)\n",
    "    axes[i].plot([y_test_reg.min(), y_test_reg.max()], \n",
    "                 [y_test_reg.min(), y_test_reg.max()], 'r--', linewidth=2)\n",
    "    axes[i].set_xlabel('Actual Values')\n",
    "    axes[i].set_ylabel('Predicted Values')\n",
    "    axes[i].set_title(f'{name}\\nR² = {r2:.4f}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "for i, (name, results) in enumerate(reg_results.items()):\n",
    "    y_pred = results['predictions']\n",
    "    residuals = y_test_reg - y_pred\n",
    "    \n",
    "    axes[i].scatter(y_pred, residuals, alpha=0.6)\n",
    "    axes[i].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[i].set_xlabel('Predicted Values')\n",
    "    axes[i].set_ylabel('Residuals')\n",
    "    axes[i].set_title(f'{name} - Residual Plot')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8905b393",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling and Preprocessing\n",
    "\n",
    "Demonstrate the importance of proper train-test split when applying preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b77d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with features of different scales\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Features with different scales\n",
    "feature1 = np.random.normal(100, 15, n_samples)  # Large scale\n",
    "feature2 = np.random.normal(5, 2, n_samples)     # Medium scale  \n",
    "feature3 = np.random.normal(0.01, 0.005, n_samples)  # Small scale\n",
    "\n",
    "X_scale = np.column_stack([feature1, feature2, feature3])\n",
    "y_scale = (feature1 * 0.5 + feature2 * 2 + feature3 * 1000 + \n",
    "           np.random.normal(0, 10, n_samples))\n",
    "\n",
    "print(\"Original Feature Statistics:\")\n",
    "print(f\"Feature 1 - Mean: {feature1.mean():.2f}, Std: {feature1.std():.2f}\")\n",
    "print(f\"Feature 2 - Mean: {feature2.mean():.2f}, Std: {feature2.std():.2f}\")\n",
    "print(f\"Feature 3 - Mean: {feature3.mean():.4f}, Std: {feature3.std():.4f}\")\n",
    "print()\n",
    "\n",
    "# Split the data first\n",
    "X_train_scale, X_test_scale, y_train_scale, y_test_scale = train_test_split(\n",
    "    X_scale, y_scale, test_size=0.2, random_state=42)\n",
    "\n",
    "# CORRECT WAY: Fit scaler on training data only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_scale)\n",
    "X_test_scaled = scaler.transform(X_test_scale)  # Only transform, don't fit\n",
    "\n",
    "print(\"After Scaling - Training Set Statistics:\")\n",
    "for i in range(X_train_scaled.shape[1]):\n",
    "    mean = X_train_scaled[:, i].mean()\n",
    "    std = X_train_scaled[:, i].std()\n",
    "    print(f\"Feature {i+1} - Mean: {mean:.4f}, Std: {std:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"After Scaling - Test Set Statistics:\")\n",
    "for i in range(X_test_scaled.shape[1]):\n",
    "    mean = X_test_scaled[:, i].mean()\n",
    "    std = X_test_scaled[:, i].std()\n",
    "    print(f\"Feature {i+1} - Mean: {mean:.4f}, Std: {std:.4f}\")\n",
    "print()\n",
    "\n",
    "# Compare models with and without scaling\n",
    "# Without scaling\n",
    "lr_no_scale = LinearRegression()\n",
    "lr_no_scale.fit(X_train_scale, y_train_scale)\n",
    "y_pred_no_scale = lr_no_scale.predict(X_test_scale)\n",
    "r2_no_scale = r2_score(y_test_scale, y_pred_no_scale)\n",
    "\n",
    "# With scaling\n",
    "lr_scaled = LinearRegression()\n",
    "lr_scaled.fit(X_train_scaled, y_train_scale)\n",
    "y_pred_scaled = lr_scaled.predict(X_test_scaled)\n",
    "r2_scaled = r2_score(y_test_scale, y_pred_scaled)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(f\"Without scaling - R² Score: {r2_no_scale:.4f}\")\n",
    "print(f\"With scaling - R² Score: {r2_scaled:.4f}\")\n",
    "print()\n",
    "\n",
    "# Visualize the effect of scaling\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original features\n",
    "for i in range(3):\n",
    "    axes[0, i].hist(X_scale[:, i], bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[0, i].set_title(f'Original Feature {i+1}')\n",
    "    axes[0, i].set_ylabel('Frequency')\n",
    "\n",
    "# Scaled features\n",
    "X_all_scaled = scaler.fit_transform(X_scale)  # For visualization only\n",
    "for i in range(3):\n",
    "    axes[1, i].hist(X_all_scaled[:, i], bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[1, i].set_title(f'Scaled Feature {i+1}')\n",
    "    axes[1, i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show coefficients comparison\n",
    "print(\"Model Coefficients Comparison:\")\n",
    "print(f\"Without scaling: {lr_no_scale.coef_}\")\n",
    "print(f\"With scaling: {lr_scaled.coef_}\")\n",
    "print()\n",
    "print(\"Note: Scaling makes coefficients more comparable and often improves model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aaa54a",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning with Proper Validation\n",
    "\n",
    "Demonstrate hyperparameter tuning using GridSearchCV while maintaining proper train-test separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the iris dataset for hyperparameter tuning\n",
    "X_iris, y_iris = load_iris(return_X_y=True)\n",
    "\n",
    "# Split into train and test sets (test set is held out completely)\n",
    "X_train_hp, X_test_hp, y_train_hp, y_test_hp = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.2, stratify=y_iris, random_state=42)\n",
    "\n",
    "print(\"Hyperparameter Tuning Demonstration\")\n",
    "print(f\"Training set: {X_train_hp.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_hp.shape[0]} samples\")\n",
    "print()\n",
    "\n",
    "# Define hyperparameter grids for different models\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 7, None],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'kernel': ['linear', 'rbf', 'poly'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for model_name, config in param_grids.items():\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    \n",
    "    # Perform grid search with cross-validation on training set only\n",
    "    grid_search = GridSearchCV(\n",
    "        config['model'], \n",
    "        config['params'], \n",
    "        cv=5, \n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Fit on training data only\n",
    "    grid_search.fit(X_train_hp, y_train_hp)\n",
    "    \n",
    "    # Store the best model\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_score = grid_search.best_estimator_.score(X_test_hp, y_test_hp)\n",
    "    print(f\"Test set score: {test_score:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Compare all models on the test set\n",
    "print(\"Final Model Comparison on Test Set:\")\n",
    "test_scores = {}\n",
    "for model_name, model in best_models.items():\n",
    "    test_score = model.score(X_test_hp, y_test_hp)\n",
    "    test_scores[model_name] = test_score\n",
    "    print(f\"{model_name}: {test_score:.4f}\")\n",
    "\n",
    "# Visualize hyperparameter tuning results\n",
    "plt.figure(figsize=(10, 6))\n",
    "models = list(test_scores.keys())\n",
    "scores = list(test_scores.values())\n",
    "\n",
    "bars = plt.bar(models, scores, color=['skyblue', 'lightcoral'], edgecolor='black')\n",
    "plt.title('Model Performance After Hyperparameter Tuning', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Test Accuracy', fontsize=14)\n",
    "plt.ylim(0.8, 1.0)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "             f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance for the best Random Forest model\n",
    "if 'Random Forest' in best_models:\n",
    "    rf_model = best_models['Random Forest']\n",
    "    feature_names = load_iris().feature_names\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance = rf_model.feature_importances_\n",
    "    sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "    \n",
    "    plt.bar(range(len(feature_importance)), feature_importance[sorted_idx], \n",
    "            color='lightgreen', edgecolor='black')\n",
    "    plt.xticks(range(len(feature_importance)), \n",
    "               [feature_names[i] for i in sorted_idx], rotation=45)\n",
    "    plt.title('Feature Importance - Best Random Forest Model', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('Importance', fontsize=14)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. Always split your data BEFORE any preprocessing or hyperparameter tuning\")\n",
    "print(\"2. Use cross-validation on the training set for hyperparameter tuning\")\n",
    "print(\"3. Keep the test set completely separate until final evaluation\")\n",
    "print(\"4. The test set provides an unbiased estimate of model performance\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
